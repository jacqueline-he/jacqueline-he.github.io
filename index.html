<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jacqueline He</title>
  
  <meta name="author" content="Jacqueline He">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŒ±</text></svg>">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-JDY1XTPZQ6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-JDY1XTPZQ6');
  </script>
  <script type="text/javascript">
    function showAbstract(abstract)
    {
    	var x = document.getElementById(abstract);
    	if (x.style.display === "none") {
      		x.style.display = "inline-block";
    	}
    	else {
      		x.style.display = "none";
    	}
    }
    
    </script>
</head>

<body>
  <table style="width:100%;max-width:650px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jacqueline He</name>
              </p>
              <p>I am a first-year Ph.D. student in the <a href="https://www.cs.washington.edu/research/nlp">Natural Language Processing</a> group at the University of Washington. My advisor is <a href="https://www.cs.washington.edu/people/faculty/lsz/">Luke Zettlemoyer</a>. I am fortunate to be supported by the <a href="https://www.nsfgrfp.org/">NSF Graduate Research Fellowship.</a></p>
              <p> In 2022, I graduated <i>summa cum laude</i> from Princeton University with a B.S.E. in <a href="https://cs.princeton.edu/">Computer Science</a>, and minors in <a href="https://bcf.princeton.edu">Finance</a> and <a href="https://csml.princeton.edu/">Statistics & Machine Learning</a>.  I was affiliated with the <a href="https://princeton-nlp.github.io/">Princeton Natural Language Processing</a> group, where my primary advisor was <a href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>. In the interim between undergrad and grad school, I worked as a software engineer at Meta. 
                 </p>
                <p>I grew up in San Jose, California, and was born in Tucson, Arizona.</p>
              </p>
              <p style="text-align:center">
                <a href="mailto:jyyh@cs.washington.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=krTnRRUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://linkedin.com/in/jacqueline-he">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/jacqueline-he/">Github</a> &nbsp/&nbsp
		<a href="https://twitter.com/jcqln_h/">Twitter</a>
              </p>
            </td>
            <!--
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/JonBarron.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/JonBarron_circle.jpg" class="hoverZoomLink"></a>
            </td>
            -->
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am broadly interested in deep learning and natural language processing, specifically the emergent capabilities, applications, and risks of large language models. 
              </p>
		<p>Thanks to my amazing mentors and collaborators! â˜º </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <papertitle>Challenges in Context-Aware Neural Machine Translation</papertitle>
            <br>
            Linghao Jin*, <strong>Jacqueline He</strong>*, Jonathan May, Xuezhe Ma
            <br>
            arXiv preprint  
            <br>
            <a id="sample_text" onClick="showAbstract('abs-challenges')">abstract</a> /
            <a href="https://arxiv.org/abs/2305.13751">paper</a>  
            <br>	
            <div id="abs-challenges" style="display:none; border-style:dotted; padding:10px;  margin-top: 10px; border-color: #8fa19c">Context-aware neural machine translation involves leveraging information beyond sentence-level context to resolve inter-sentential discourse dependencies and improve document-level translation quality, and has given rise to a number of recent techniques. However, despite well-reasoned intuitions, most context-aware translation models show only modest improvements over sentence-level systems. In this work, we investigate several challenges that impede progress within this field, relating to discourse phenomena, context usage, model architectures, and document-level evaluation. To address these problems, we propose a more realistic setting for document-level translation, called paragraph-to-paragraph (para2para) translation, and collect a new dataset of Chinese-English novels to promote future research.</div>
          </td>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <papertitle>MABEL: Attenuating Gender Bias using Textual Entailment Data</papertitle>
            <br>
            <strong>Jacqueline He</strong>, Mengzhou Xia, Christiane Fellbaum, Danqi Chen
            <br>
            <em>EMNLP</em> 2022 
            <br>
            <a id="sample_text" onClick="showAbstract('abs-mabel')">abstract</a> /
	    <a href="https://arxiv.org/abs/2210.14975">paper</a> / 
            <a href="https://github.com/princeton-nlp/MABEL">code</a> 
            <br>	 
            <div id="abs-mabel" style="display:none; border-style:dotted; padding:10px;  margin-top: 10px; border-color: #8fa19c">Pre-trained language models encode undesirable social biases, which are further exacerbated in downstream use. To this end, we propose MABEL (a Method for Attenuating Gender Bias using Entailment Labels), an intermediate pre-training approach for mitigating gender bias in contextualized representations. Key to our approach is the use of a contrastive learning objective on counterfactually augmented, gender-balanced entailment pairs from natural language inference (NLI) datasets. We also introduce an alignment regularizer that pulls identical entailment pairs along opposite gender directions closer. We extensively evaluate our approach on intrinsic and extrinsic metrics, and show that MABEL outperforms previous task-agnostic debiasing approaches in terms of fairness. It also preserves task performance after fine-tuning on downstream tasks. Together, these findings demonstrate the suitability of NLI data as an effective means of bias mitigation, as opposed to only using unlabeled sentences in the literature. Finally, we identify that existing approaches often use evaluation settings that are insufficient or inconsistent. We make an effort to reproduce and compare previous methods, and call for unifying the evaluation settings across gender debiasing methods for better future comparison.</div>
          </td>

        </tbody>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <papertitle>Can Rationalization Improve Robustness?</papertitle>
            <br>
            Howard Chen, <strong>Jacqueline He</strong>, Karthik Narasimhan, Danqi Chen
            <br>
            <em>NAACL</em> 2022  
            <br>
            <a id="sample_text" onClick="showAbstract('abs-rationale')">abstract</a> /
            <a href="https://arxiv.org/abs/2204.11790">paper</a> / 
            <a href="https://github.com/princeton-nlp/rationale-robustness">code</a>  
            <br>	
            <div id="abs-rationale" style="display:none; border-style:dotted; padding:10px;  margin-top: 10px; border-color: #8fa19c">A growing line of work has investigated the development of neural NLP models that can produce rationales--subsets of input that can explain their model predictions. In this paper, we ask whether such rationale models can also provide robustness to adversarial attacks in addition to their interpretable nature. Since these models need to first generate rationales ("rationalizer") before making predictions ("predictor"), they have the potential to ignore noise or adversarially added text by simply masking it out of the generated rationale. To this end, we systematically generate various types of 'AddText' attacks for both token and sentence-level rationalization tasks, and perform an extensive empirical evaluation of state-of-the-art rationale models across five different tasks. Our experiments reveal that the rationale models show the promise to improve robustness, while they struggle in certain scenarios--when the rationalizer is sensitive to positional bias or lexical choices of attack text. Further, leveraging human rationale as supervision does not always translate to better performance. Our study is a first step towards exploring the interplay between interpretability and robustness in the rationalize-then-predict framework.</div>
          </td>

        </tbody>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;"> This page was adapted from <a href="https://jonbarron.info/">Jon Barron's</a> template. </p>
            </td>
          </tr>
        </tbody></table>
        
      </td>
    </tr>
  </table>

</body>

</html>
