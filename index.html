<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jacqueline He</title>

  <meta name="author" content="Jacqueline He">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon"
    href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŒ±</text></svg>">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-JDY1XTPZQ6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-JDY1XTPZQ6');
  </script>
  <script type="text/javascript">
    function showAbstract(abstract) {
      var x = document.getElementById(abstract);
      if (x.style.display === "none") {
        x.style.display = "inline-block";
      }
      else {
        x.style.display = "none";
      }
    }

  </script>
</head>

<body>
  <table
    style="width:100%;max-width:650px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Jacqueline He</name>
                  </p>
                  <p>I am a second-year Ph.D. student in the <a href="https://www.cs.washington.edu/research/nlp">Natural
                      Language Processing</a> group at the University of Washington. My advisors are <a
                      href="https://www.cs.washington.edu/people/faculty/lsz/">Luke Zettlemoyer</a> and <a
                      href="https://www.koh.pw/">Pang Wei Koh</a>. I am fortunate to
                    be supported by the <a href="https://www.nsfgrfp.org/">NSF Graduate Research Fellowship</a>.</p>
                  <p> In 2022, I graduated <i>summa cum laude</i> from Princeton University with a B.S.E. in <a
                      href="https://cs.princeton.edu/">Computer Science</a>, and minors in <a
                      href="https://bcf.princeton.edu">Finance</a> and <a href="https://csml.princeton.edu/">Statistics
                      & Machine Learning</a>. I was affiliated with the <a
                      href="https://princeton-nlp.github.io/">Princeton Natural Language Processing</a> group, where my
                    primary advisor was <a href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>. In the interim
                    between undergrad and grad school, I worked as a software engineer at Meta.
                  </p>
                  <p>I grew up in San Jose, California, and was born in Tucson, Arizona.</p>
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:jyyh@cs.washington.edu">Email</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=krTnRRUAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/jacqueline-he/">Github</a> &nbsp/&nbsp
                    <a href="https://twitter.com/jcqln_h/">Twitter</a>
                  </p>
                </td>
                <!--
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/JonBarron.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/JonBarron_circle.jpg" class="hoverZoomLink"></a>
            </td>
            -->
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Research</heading>
                  <p>
                    I am broadly interested in natural language processing, specifically how to encourage controllable behaviors in language models.
                  </p>
                  <p>Thanks to my amazing mentors and collaborators! â˜º </p>
                  <p><i> * denotes equal contribution</i></p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <papertitle>Precise Information Control in Long-Form Text Generation</papertitle>
                    <br>
                    <strong>Jacqueline He</strong>, Howard Yen*, Margaret Li*, Stella Li, Zhiyuan Zeng, Weijia Shi, Yulia Tsvetkov, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer
                    <br>
                    <em>preprint</em> 2025
                    <br>
                    <a id="sample_text" onClick="showAbstract('abs-pic')">abstract</a> /
                    <a href="https://arxiv.org/abs/2506.06589">paper</a> /
                    <a href="https://github.com/jacqueline-he/precise-information-control">code</a> 
                    <br>
                    <div id="abs-pic"
                      style="display:none; border-style:dotted; padding:10px;  margin-top: 10px; border-color: #8fa19c">
                      A central challenge in modern language models (LMs) is intrinsic hallucination: the generation of information that is plausible but unsubstantiated relative to input context. To study this problem, we propose Precise Information Control (PIC), a new task formulation that requires models to generate long-form outputs grounded in a provided set of short self-contained statements, known as verifiable claims, without adding any unsupported ones. For comprehensiveness, PIC includes a full setting that tests a model's ability to include exactly all input claims, and a partial setting that requires the model to selectively incorporate only relevant claims. We present PIC-Bench, a benchmark of eight long-form generation tasks (e.g., summarization, biography generation) adapted to the PIC setting, where LMs are supplied with well-formed, verifiable input claims. Our evaluation of a range of open and proprietary LMs on PIC-Bench reveals that, surprisingly, state-of-the-art LMs still intrinsically hallucinate in over 70% of outputs. To alleviate this lack of faithfulness, we introduce a post-training framework, using a weakly supervised preference data construction method, to train an 8B PIC-LM with stronger PIC ability--improving from 69.1% to 91.0% F1 in the full PIC setting. When integrated into end-to-end factual generation pipelines, PIC-LM improves exact match recall by 17.1% on ambiguous QA with retrieval, and factual precision by 30.5% on a birthplace verification task, underscoring the potential of precisely grounded generation.
                      </div>
                  </td>
                </tbody>

          <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <papertitle>OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs</papertitle>
                    <br>
                    Akari Asai, <strong>Jacqueline He</strong>*, Rulin Shao*, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike D'arcy, David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke Zettlemoyer, Graham Neubig, Dan Weld, Doug Downey, Wen-tau Yih, Pang Wei Koh, Hannaneh Hajishirzi
                    <br>
                    <em>preprint</em> 2024
                    <br>
                    <a id="sample_text" onClick="showAbstract('abs-openscholar')">abstract</a> /
                    <a href="https://arxiv.org/abs/2411.14199">paper</a> /
                    <a href="https://github.com/AkariAsai/OpenScholar">code</a> /
                    <a href="https://allenai.org/blog/openscholar">blog</a> /
                    <a href="https://openscholar.allen.ai/">demo</a>
                    <br>
                    <div id="abs-openscholar"
                      style="display:none; border-style:dotted; padding:10px;  margin-top: 10px; border-color: #8fa19c">
                      Scientific progress depends on researchers' ability to synthesize the growing body of literature. Can large language models (LMs) assist scientists in this task? We introduce OpenScholar, a specialized retrieval-augmented LM that answers scientific queries by identifying relevant passages from 45 million open-access papers and synthesizing citation-backed responses. To evaluate OpenScholar, we develop ScholarQABench, the first large-scale multi-domain benchmark for literature search, comprising 2,967 expert-written queries and 208 long-form answers across computer science, physics, neuroscience, and biomedicine. On ScholarQABench, OpenScholar-8B outperforms GPT-4o by 5% and PaperQA2 by 7% in correctness, despite being a smaller, open model. While GPT4o hallucinates citations 78 to 90% of the time, OpenScholar achieves citation accuracy on par with human experts. OpenScholar's datastore, retriever, and self-feedback inference loop also improves off-the-shelf LMs: for instance, OpenScholar-GPT4o improves GPT-4o's correctness by 12%. In human evaluations, experts preferred OpenScholar-8B and OpenScholar-GPT4o responses over expert-written ones 51% and 70% of the time, respectively, compared to GPT4o's 32%. We open-source all of our code, models, datastore, data and a public demo.</div>
                  </td>
                </tbody>

          <table
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <papertitle>Scaling Retrieval-Based Language Models with a Trillion-Token Datastore</papertitle>
              <br>
              Rulin Shao, <strong>Jacqueline He</strong>, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke Zettlemoyer, Pang Wei Koh
              <br>
              <em>NeurIPS</em> 2024
              <br>
              <a id="sample_text" onClick="showAbstract('abs-massiveds')">abstract</a> /
              <a href="https://arxiv.org/abs/2407.12854">paper</a> /
              <a href="https://github.com/RulinShao/retrieval-scaling">code</a>
              <br>
              <div id="abs-massiveds"
                style="display:none; border-style:dotted; padding:10px;  margin-top: 10px; border-color: #8fa19c">
                Scaling laws with respect to the amount of training data and the number of parameters allow us to predict the cost-benefit trade-offs of pretraining language models (LMs) in different configurations. In this paper, we consider another dimension of scaling: the amount of data available at inference time. Specifically, we find that increasing the size of the datastore used by a retrieval-based LM monotonically improves language modeling and several downstream tasks without obvious saturation, such that a smaller model augmented with a large datastore outperforms a larger LM-only model on knowledge-intensive tasks. By plotting compute-optimal scaling curves with varied datastore, model, and pretraining data sizes, we show that using larger datastores can significantly improve model performance for the same training compute budget. We carry out our study by constructing a 1.4 trillion-token datastore named MassiveDS, which is the largest and the most diverse open-sourced datastore for retrieval-based LMs to date, and designing an efficient pipeline for studying datastore scaling in a computationally accessible manner. Finally, we analyze the effect of improving the retriever, datastore quality filtering, and other design choices on our observed scaling trends. Overall, our results show that datastore size should be considered as an integral part of LM efficiency and performance trade-offs. To facilitate future research, we open-source our datastore and code at https://github.com/RulinShao/retrieval-scaling.
            </td>
          </tbody>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <papertitle>Challenges in Context-Aware Neural Machine Translation</papertitle>
                <br>
                Linghao Jin*, <strong>Jacqueline He</strong>*, Jonathan May, Xuezhe Ma
                <br>
                <em>EMNLP</em> 2023
                <br>
                <a id="sample_text" onClick="showAbstract('abs-challenges')">abstract</a> /
                <a href="https://arxiv.org/abs/2305.13751">paper</a> /
                <a href="https://github.com/Linghao-Jin/canmt-challenges">code</a>
                <br>
                <div id="abs-challenges"
                  style="display:none; border-style:dotted; padding:10px;  margin-top: 10px; border-color: #8fa19c">
                  Context-aware neural machine translation involves leveraging information beyond sentence-level context
                  to resolve inter-sentential discourse dependencies and improve document-level translation quality, and
                  has given rise to a number of recent techniques. However, despite well-reasoned intuitions, most
                  context-aware translation models show only modest improvements over sentence-level systems. In this
                  work, we investigate several challenges that impede progress within this field, relating to discourse
                  phenomena, context usage, model architectures, and document-level evaluation. To address these
                  problems, we propose a more realistic setting for document-level translation, called
                  paragraph-to-paragraph (para2para) translation, and collect a new dataset of Chinese-English novels to
                  promote future research.</div>
              </td>

              <table
                style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <papertitle>MABEL: Attenuating Gender Bias using Textual Entailment Data</papertitle>
                    <br>
                    <strong>Jacqueline He</strong>, Mengzhou Xia, Christiane Fellbaum, Danqi Chen
                    <br>
                    <em>EMNLP</em> 2022
                    <br>
                    <a id="sample_text" onClick="showAbstract('abs-mabel')">abstract</a> /
                    <a href="https://arxiv.org/abs/2210.14975">paper</a> /
                    <a href="https://github.com/princeton-nlp/MABEL">code</a>
                    <br>
                    <div id="abs-mabel"
                      style="display:none; border-style:dotted; padding:10px;  margin-top: 10px; border-color: #8fa19c">
                      Pre-trained language models encode undesirable social biases, which are further exacerbated in
                      downstream use. To this end, we propose MABEL (a Method for Attenuating Gender Bias using
                      Entailment Labels), an intermediate pre-training approach for mitigating gender bias in
                      contextualized representations. Key to our approach is the use of a contrastive learning objective
                      on counterfactually augmented, gender-balanced entailment pairs from natural language inference
                      (NLI) datasets. We also introduce an alignment regularizer that pulls identical entailment pairs
                      along opposite gender directions closer. We extensively evaluate our approach on intrinsic and
                      extrinsic metrics, and show that MABEL outperforms previous task-agnostic debiasing approaches in
                      terms of fairness. It also preserves task performance after fine-tuning on downstream tasks.
                      Together, these findings demonstrate the suitability of NLI data as an effective means of bias
                      mitigation, as opposed to only using unlabeled sentences in the literature. Finally, we identify
                      that existing approaches often use evaluation settings that are insufficient or inconsistent. We
                      make an effort to reproduce and compare previous methods, and call for unifying the evaluation
                      settings across gender debiasing methods for better future comparison.</div>
                  </td>
                </tbody>
                <table
                  style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                  <tbody>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                      <papertitle>Can Rationalization Improve Robustness?</papertitle>
                      <br>
                      Howard Chen, <strong>Jacqueline He</strong>, Karthik Narasimhan, Danqi Chen
                      <br>
                      <em>NAACL</em> 2022
                      <br>
                      <a id="sample_text" onClick="showAbstract('abs-rationale')">abstract</a> /
                      <a href="https://arxiv.org/abs/2204.11790">paper</a> /
                      <a href="https://github.com/princeton-nlp/rationale-robustness">code</a>
                      <br>
                      <div id="abs-rationale"
                        style="display:none; border-style:dotted; padding:10px;  margin-top: 10px; border-color: #8fa19c">
                        A growing line of work has investigated the development of neural NLP models that can produce
                        rationales--subsets of input that can explain their model predictions. In this paper, we ask
                        whether such rationale models can also provide robustness to adversarial attacks in addition to
                        their interpretable nature. Since these models need to first generate rationales
                        ("rationalizer") before making predictions ("predictor"), they have the potential to ignore
                        noise or adversarially added text by simply masking it out of the generated rationale. To this
                        end, we systematically generate various types of 'AddText' attacks for both token and
                        sentence-level rationalization tasks, and perform an extensive empirical evaluation of
                        state-of-the-art rationale models across five different tasks. Our experiments reveal that the
                        rationale models show the promise to improve robustness, while they struggle in certain
                        scenarios--when the rationalizer is sensitive to positional bias or lexical choices of attack
                        text. Further, leveraging human rationale as supervision does not always translate to better
                        performance. Our study is a first step towards exploring the interplay between interpretability
                        and robustness in the rationalize-then-predict framework.</div>
                    </td>

                  </tbody>
                  <table
                    style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                      <tr>
                        <td style="padding:0px">
                          <br>
                          <p style="text-align:right;font-size:small;"> Page template from <a
                              href="https://jonbarron.info/">Jon Barron</a>.</p>
                        </td>
                      </tr>
                    </tbody>
                  </table>

        </td>
      </tr>
  </table>

</body>

</html>